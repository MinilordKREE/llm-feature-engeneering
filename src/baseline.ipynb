{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /data/chenxi/anaconda3/envs/myenv did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/data/chenxi/anaconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from prompt.response_prompt import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = arff_to_dataframe(\"/data/chenxi/llm-feature-engeneering/dataset/diabetes.arff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models= {\n",
    "    # 'Logistic Regression': LogisticRegression(max_iter=1000000),\n",
    "    # 'K-Nearest Neighbors': KNeighborsClassifier(algorithm='ball_tree'),\n",
    "    # 'Naive Bayes': GaussianNB(),\n",
    "    # 'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    # 'AdaBoost': AdaBoostClassifier(),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    # 'Support Vector Machine': SVC(probability=True),  # Enable probability estimates\n",
    "    # 'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['analysis'] = pd.read_csv('analysis.csv')\n",
    "df['sum']= pd.read_csv('sum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = df.apply(lambda row: template.format(**row), axis=1)\n",
    "# df['analysis']= prompts.apply(lambda x: decoder_for_gpt3(x, max_length = 1000))\n",
    "# # Save the dataframe with the generated summaries to a new CSV file\n",
    "# df['analysis'].to_csv('analysis.csv', index=False)\n",
    "# prompts_sum = df['analysis'].apply(lambda row: template_for_sum.format(analysis=row))\n",
    "# df['sum'] = prompts_sum.apply(lambda x: decoder_for_gpt3(x, max_length = 1000))\n",
    "# df['sum'].to_csv('sum.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0              6      148             72             35        0  33.6   \n",
      "1              1       85             66             29        0  26.6   \n",
      "2              8      183             64              0        0  23.3   \n",
      "3              1       89             66             23       94  28.1   \n",
      "4              0      137             40             35      168  43.1   \n",
      "..           ...      ...            ...            ...      ...   ...   \n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                       0.627   50        1  \n",
      "1                       0.351   31        0  \n",
      "2                       0.672   32        1  \n",
      "3                       0.167   21        0  \n",
      "4                       2.288   33        1  \n",
      "..                        ...  ...      ...  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "\n",
      "[768 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "def get_matching_cols(df, regex):\n",
    "    r = re.compile(regex)\n",
    "    return list(filter(r.match, df.columns))\n",
    "\n",
    "def get_embedding_cols(df):\n",
    "    return get_matching_cols(df, \"(vec_\\d+)\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
    "\n",
    "def explode(col, prefix):\n",
    "    n_cols = len(col[0])\n",
    "    col_names = [prefix + str(i) for i in range(n_cols)]\n",
    "    return pd.DataFrame(col.to_list(), columns=col_names)\n",
    "\n",
    "# Methods for feature engineering\n",
    "def method_baseline(df):\n",
    "    X = df[[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]]\n",
    "    y = df[\"num\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_final = scaler.fit_transform(X)\n",
    "    return X_final, y\n",
    "\n",
    "def method_SelectK(df):\n",
    "    y = df[\"num\"]\n",
    "    X_cat = df[[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]]\n",
    "    embed_cols = get_embedding_cols(df)\n",
    "    X_text = df[embed_cols]\n",
    "    X_comb = pd.concat([X_cat, X_text], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_comb)\n",
    "    \n",
    "    # Define a range of possible values of k\n",
    "    possible_k_values = list(range(1, 50))\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_k = None\n",
    "    best_features = None\n",
    "\n",
    "    model = SVC(probability=True)\n",
    "\n",
    "    \n",
    "    for k in possible_k_values:\n",
    "        # Use SelectKBest to select top k features\n",
    "        selector = SelectKBest(mutual_info_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X_scaled, y)\n",
    "        \n",
    "        # Train and evaluate the model using cross-validation\n",
    "        score = cross_val_score(model, X_selected, y, cv=5, scoring='roc_auc').mean()\n",
    "        \n",
    "        # If the score is the best so far, update best_k, best_score, and best_features\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            best_features = X_selected\n",
    "\n",
    "    # Combine the categorical data with the best selected features\n",
    "    X_final = pd.concat([X_cat, pd.DataFrame(best_features)], axis=1)\n",
    "    X_final.columns = X_final.columns.astype(str)\n",
    "    print(best_k)\n",
    "    \n",
    "    return X_final, y # Also return the best k for information\n",
    "\n",
    "\n",
    "\n",
    "def method_PCA(df):\n",
    "    \n",
    "    # Splitting X and y\n",
    "    X = df.drop('num', axis=1)\n",
    "    y = df['num']\n",
    "\n",
    "    # Separate original categorical features\n",
    "    X_cat = df[[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]]\n",
    "    # Extract the text embeddings\n",
    "    embed_cols = get_embedding_cols(X)\n",
    "    X_text = X[embed_cols]\n",
    "\n",
    "    # Combine the embeddings and the original set\n",
    "    X_comb = pd.concat([X_cat, X_text], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_comb)\n",
    "\n",
    "    # Applying PCA on the combined data\n",
    "    best_n_components = None\n",
    "    best_score = float('-inf')\n",
    "    for n_components in range(1, 50):  # Checking all possible number of components\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        # Train a model (e.g., logistic regression) on the PCA components and compute the performance\n",
    "        model = LogisticRegression()\n",
    "        score = cross_val_score(model, X_pca, y, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_components = n_components\n",
    "\n",
    "    # Use PCA with the best number of components\n",
    "    pca = PCA(n_components=best_n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Combining PCA components with original categorical data\n",
    "    X_final = pd.concat([X_cat, pd.DataFrame(X_pca)], axis=1)\n",
    "    X_final.columns = X_final.columns.astype(str)\n",
    "\n",
    "    return X_final, y\n",
    "\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_models(df, models, methods):\n",
    "    # logic to evaluate models with provided methods\n",
    "    colors = ['black', 'green', 'blue', 'red']\n",
    "\n",
    "    for metric in ['accuracy', 'roc_auc']:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Iterate through methods\n",
    "        for i, method in enumerate(methods):\n",
    "            if method == 'baseline':\n",
    "                X_final, y = method_baseline(df)\n",
    "            elif method == 'PCA':\n",
    "                X_final, y = method_PCA(df)\n",
    "            elif method == 'SelectK':\n",
    "                X_final, y = method_SelectK(df)\n",
    "        \n",
    "            # ... (rest of the evaluation function logic)\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            performance_metrics = {metric: {model_name: cross_val_score(model, X_final, y, cv=kfold, scoring=metric) for model_name, model in models.items()}}\n",
    "\n",
    "            for name, scores in performance_metrics[metric].items():\n",
    "                print(f'{name}: {metric}: {scores.mean()} ± {scores.std()}')\n",
    "\n",
    "            x_ticks_positions = np.arange(len(models)) + i * 0.2\n",
    "            plt.boxplot([performance_metrics[metric][model_name] for model_name in models.keys()], positions=x_ticks_positions, widths=0.2, patch_artist=True,\n",
    "                        boxprops=dict(facecolor=colors[i], color=colors[i], alpha=0.6),\n",
    "                        capprops=dict(color=colors[i]),\n",
    "                        whiskerprops=dict(color=colors[i]),\n",
    "                        flierprops=dict(color=colors[i], markeredgecolor=colors[i]),\n",
    "                        medianprops=dict(color='black'))\n",
    "\n",
    "        plt.legend(handles=[mpatches.Patch(color=colors[i], label=methods[i]) for i in range(len(methods))], loc='upper right')\n",
    "        plt.title(f\"Model performance ({metric})\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.xticks(ticks=np.arange(len(models)), labels=models.keys())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "methods = ['baseline', 'SelectK', 'PCA']\n",
    "evaluate_models(df, models, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
